{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2616a7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 10:33:36.539497: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained ResNet-50 model\n",
    "model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b2d27e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "# Prepare the images\n",
    "img1 = prepare_image('./Downloads/profile_PIX_.JPG')\n",
    "img2 = prepare_image('./Downloads/image_.jpeg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f960147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 155ms/step\n"
     ]
    }
   ],
   "source": [
    "# Extract embeddings\n",
    "embedding1 = model.predict(img1)\n",
    "embedding2 = model.predict(img2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0118456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: [[0.62556916]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute similarity between the embeddings\n",
    "similarity = cosine_similarity(embedding1, embedding2)\n",
    "print(f\"Similarity: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ee149d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 142ms/step\n",
      "Test Similarity: [[0.6281172]]\n"
     ]
    }
   ],
   "source": [
    "# Prepare and extract embedding for a test image (another instance of img1)\n",
    "test_img = prepare_image('./Downloads/IMG-0316.jpg')\n",
    "test_embedding = model.predict(test_img)\n",
    "\n",
    "# Compute similarity with img1\n",
    "test_similarity = cosine_similarity(test_embedding, embedding1)\n",
    "print(f\"Test Similarity: {test_similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b772687f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Get the face encodings for each image\u001b[39;00m\n\u001b[1;32m      8\u001b[0m img1_encoding \u001b[38;5;241m=\u001b[39m face_recognition\u001b[38;5;241m.\u001b[39mface_encodings(img1)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m img2_encoding \u001b[38;5;241m=\u001b[39m \u001b[43mface_recognition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_encodings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Compare the faces and return the result\u001b[39;00m\n\u001b[1;32m     12\u001b[0m results \u001b[38;5;241m=\u001b[39m face_recognition\u001b[38;5;241m.\u001b[39mcompare_faces([img1_encoding], img2_encoding)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "\n",
    "# Load the images into numpy arrays\n",
    "img1 = face_recognition.load_image_file('./Downloads/profile_PIX_.JPG')\n",
    "img2 = face_recognition.load_image_file('./Downloads/img_test2.jpeg')\n",
    "\n",
    "# Get the face encodings for each image\n",
    "img1_encoding = face_recognition.face_encodings(img1)[0]\n",
    "img2_encoding = face_recognition.face_encodings(img2)[0]\n",
    "\n",
    "# Compare the faces and return the result\n",
    "results = face_recognition.compare_faces([img1_encoding], img2_encoding)\n",
    "\n",
    "if results[0]:\n",
    "    print(\"It's a match!\")\n",
    "else:\n",
    "    print(\"It's not a match.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b2ffa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are these images of the same person?:  False\n"
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "# Verifying two images\n",
    "result = DeepFace.verify(img1_path = './Downloads/profile_PIX_.JPG', img2_path = './Downloads/IMG_3395.jpg')\n",
    "\n",
    "print(\"Are these images of the same person?: \", result[\"verified\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7d68078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "result = DeepFace.verify(img1_path = './Downloads/profile_PIX_.JPG', img2_path = './Downloads/IMG_3395.jpg',model_name = \"VGG-Face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2041fab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are these images of the same person?:  False\n"
     ]
    }
   ],
   "source": [
    "print(\"Are these images of the same person?: \", result[\"verified\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc9fd70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-01-06 14:24:47 - facial_expression_model_weights.h5 will be downloaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://github.com/serengil/deepface_models/releases/download/v1.0/facial_expression_model_weights.h5\n",
      "To: /Users/mac/.deepface/weights/facial_expression_model_weights.h5\n",
      "100%|█████████████████████████████████████████████████████| 5.98M/5.98M [00:00<00:00, 50.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-01-06 14:24:51 - age_model_weights.h5 will be downloaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://github.com/serengil/deepface_models/releases/download/v1.0/age_model_weights.h5\n",
      "To: /Users/mac/.deepface/weights/age_model_weights.h5\n",
      "100%|███████████████████████████████████████████████████████| 539M/539M [00:16<00:00, 32.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-01-06 14:25:13 - gender_model_weights.h5 will be downloaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://github.com/serengil/deepface_models/releases/download/v1.0/gender_model_weights.h5\n",
      "To: /Users/mac/.deepface/weights/gender_model_weights.h5\n",
      "100%|███████████████████████████████████████████████████████| 537M/537M [00:16<00:00, 31.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-01-06 14:25:37 - race_model_single_batch.h5 will be downloaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://github.com/serengil/deepface_models/releases/download/v1.0/race_model_single_batch.h5\n",
      "To: /Users/mac/.deepface/weights/race_model_single_batch.h5\n",
      "100%|███████████████████████████████████████████████████████| 537M/537M [00:16<00:00, 32.3MB/s]\n",
      "Action: emotion: 100%|███████████████████████████████████████████| 4/4 [00:07<00:00,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'age': 24, 'region': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'face_confidence': 7.716838093590923, 'gender': {'Woman': 0.0003259293180235545, 'Man': 99.99967813491821}, 'dominant_gender': 'Man', 'race': {'asian': 2.026354639639294e-08, 'indian': 3.8975400684648775e-06, 'black': 100.0, 'white': 7.09193446156442e-12, 'middle eastern': 3.185289421006425e-12, 'latino hispanic': 2.0148016588450446e-07}, 'dominant_race': 'black', 'emotion': {'angry': 0.00826929826871492, 'disgust': 4.580646972840441e-08, 'fear': 0.00042392071009089705, 'happy': 1.0089879855513573, 'sad': 0.0895503384526819, 'surprise': 0.011559346603462473, 'neutral': 98.88120889663696}, 'dominant_emotion': 'neutral'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "# Analyzing an image\n",
    "analysis = DeepFace.analyze(img_path = './Downloads/profile_PIX_.JPG', actions = ['age', 'gender', 'race', 'emotion'])\n",
    "\n",
    "print(analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb01ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture video from the first webcam connected to your computer.\n",
    "video_capture = cv2.VideoCapture(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530810b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'emotion': {'angry': 3.5866968049817626e-05, 'disgust': 6.164990335889315e-12, 'fear': 91.3627992435933, 'happy': 3.6878052428000876, 'sad': 0.3756285662944834, 'surprise': 3.1522300061905377, 'neutral': 1.4214934949286586}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 2.9239787902481586e-05, 'disgust': 4.215516796311407e-12, 'fear': 85.69010666726749, 'happy': 4.309787087465872, 'sad': 0.17879809216954207, 'surprise': 8.475015792248751, 'neutral': 1.3462630358322816}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 3.428246646447274e-05, 'disgust': 6.640433938264007e-12, 'fear': 57.52282649006528, 'happy': 3.671013498781502, 'sad': 0.08672985262105136, 'surprise': 37.57894187224604, 'neutral': 1.1404534373636916}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00043728626767367907, 'disgust': 2.1185361943500265e-10, 'fear': 81.76228673975827, 'happy': 2.5638000036120423, 'sad': 1.0942176078351036, 'surprise': 1.5280457151911941, 'neutral': 13.051220128158354}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.002293946818099357, 'disgust': 3.108576243637806e-09, 'fear': 49.64627027511597, 'happy': 10.613250732421875, 'sad': 2.2408895194530487, 'surprise': 1.912933774292469, 'neutral': 35.584357380867004}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0002776217963348445, 'disgust': 1.2266295481890537e-10, 'fear': 78.10315359253607, 'happy': 8.860046944621024, 'sad': 0.15792390108772522, 'surprise': 6.578755025789441, 'neutral': 6.299841779461013}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00033878573049150873, 'disgust': 2.1715619753781557e-10, 'fear': 37.668463587760925, 'happy': 3.5896290093660355, 'sad': 6.319303065538406, 'surprise': 0.03287659783381969, 'neutral': 52.38938927650452}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00035472783748466625, 'disgust': 3.064630066732748e-10, 'fear': 81.91478683595972, 'happy': 5.010584292272498, 'sad': 2.01361559751854, 'surprise': 0.12472615863206181, 'neutral': 10.935919359703217}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00015292407624656335, 'disgust': 4.608374771196411e-11, 'fear': 88.81150484085083, 'happy': 0.7318222895264626, 'sad': 3.917839005589485, 'surprise': 0.0022781243387726136, 'neutral': 6.53640478849411}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00020768675312865525, 'disgust': 9.889453622333177e-11, 'fear': 85.53112745285034, 'happy': 1.047341711819172, 'sad': 3.19148451089859, 'surprise': 0.01705780014162883, 'neutral': 10.21278202533722}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0002636345785095832, 'disgust': 9.178134607734311e-11, 'fear': 89.31424490383206, 'happy': 1.650550040668254, 'sad': 2.6959038759355396, 'surprise': 0.007607868789981311, 'neutral': 6.331431875399574}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0001764000199919823, 'disgust': 9.100935067041751e-11, 'fear': 84.43474584039996, 'happy': 1.4453438902311195, 'sad': 3.0715973984119174, 'surprise': 0.011553866274324138, 'neutral': 11.036590312312878}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00024013459096750012, 'disgust': 1.164322002526752e-10, 'fear': 86.62499785423279, 'happy': 2.615712210536003, 'sad': 1.9865518435835838, 'surprise': 0.03586149832699448, 'neutral': 8.736634999513626}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00037691654597438173, 'disgust': 2.5282614543598037e-10, 'fear': 83.60558152198792, 'happy': 3.4682486206293106, 'sad': 2.614368684589863, 'surprise': 0.07554556941613555, 'neutral': 10.235879570245743}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0001987861423913273, 'disgust': 8.9344270913852e-11, 'fear': 57.35490322113037, 'happy': 0.10286795441061258, 'sad': 30.84931969642639, 'surprise': 9.28553561152512e-05, 'neutral': 11.692619323730469}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00022890728814672912, 'disgust': 2.8988736411328375e-08, 'fear': 36.37329041957855, 'happy': 0.08104424923658371, 'sad': 15.261892974376678, 'surprise': 0.006684014806523919, 'neutral': 48.276856541633606}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 6.0999406059636385e-05, 'disgust': 3.0025603531269596e-09, 'fear': 40.10172486305237, 'happy': 0.013813011173624545, 'sad': 27.659887075424194, 'surprise': 0.00033778776469262084, 'neutral': 32.224178314208984}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0003067988700422575, 'disgust': 1.1481866568630039e-07, 'fear': 40.68627059459686, 'happy': 0.1867692917585373, 'sad': 39.92650508880615, 'surprise': 0.009398835391039029, 'neutral': 19.190751016139984}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00017971124179894105, 'disgust': 4.2240499986689883e-08, 'fear': 49.2074191570282, 'happy': 0.3461335552856326, 'sad': 31.20078146457672, 'surprise': 0.03714850463438779, 'neutral': 19.20834183692932}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00014877904277454945, 'disgust': 1.031273955121037e-08, 'fear': 62.45809197425842, 'happy': 0.28910955879837275, 'sad': 27.26520597934723, 'surprise': 0.021561389439739287, 'neutral': 9.965880960226059}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00020739118810235796, 'disgust': 3.117198904882064e-08, 'fear': 56.18410632799788, 'happy': 0.4332066928497919, 'sad': 27.623402759623197, 'surprise': 0.05578200133508428, 'neutral': 15.703284358586746}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0003204863523964319, 'disgust': 1.5830386557044614e-08, 'fear': 58.398876094751124, 'happy': 0.4032259116009421, 'sad': 24.35178454826978, 'surprise': 0.08527170792910206, 'neutral': 16.760523126156517}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.000438488580154103, 'disgust': 2.542742935068905e-08, 'fear': 66.37136741025836, 'happy': 0.4077941624469563, 'sad': 20.957726722034607, 'surprise': 0.04727006617414908, 'neutral': 12.215391580415776}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0001502524786219023, 'disgust': 1.9381129801617196e-08, 'fear': 67.25163264582352, 'happy': 0.3971915928835626, 'sad': 19.202132177479992, 'surprise': 0.029007213289348084, 'neutral': 13.11988163708935}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.000635160283080287, 'disgust': 9.495048059445763e-08, 'fear': 50.54707520639, 'happy': 2.214059342258055, 'sad': 19.74657240550847, 'surprise': 1.0725765141753538, 'neutral': 26.419073174110274}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0005067531976333863, 'disgust': 5.617923683598459e-08, 'fear': 57.364735446258514, 'happy': 1.435591459160534, 'sad': 21.98573034916701, 'surprise': 0.8517700755161092, 'neutral': 18.36167087326472}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00036799817735300167, 'disgust': 2.6092683569345354e-08, 'fear': 43.84792149066925, 'happy': 2.771240845322609, 'sad': 21.30289077758789, 'surprise': 2.292724698781967, 'neutral': 29.784861207008362}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0005278400749375578, 'disgust': 6.768188232086914e-08, 'fear': 32.64017403125763, 'happy': 2.15650238096714, 'sad': 33.220988512039185, 'surprise': 0.948870088905096, 'neutral': 31.0329407453537}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0004022501823089768, 'disgust': 6.304873486607394e-08, 'fear': 30.14759482712684, 'happy': 2.0778375360772054, 'sad': 31.960175271055135, 'surprise': 0.8239890499697983, 'neutral': 34.9900078761587}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00044164430619275663, 'disgust': 4.560387623087081e-08, 'fear': 38.614240288734436, 'happy': 5.30351847410202, 'sad': 14.503346383571625, 'surprise': 6.971002370119095, 'neutral': 34.60744917392731}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0004950063612341182, 'disgust': 5.675919179104483e-08, 'fear': 47.43026793003082, 'happy': 3.6854401230812073, 'sad': 14.153575897216797, 'surprise': 4.879216104745865, 'neutral': 29.850998520851135}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0006471115284512052, 'disgust': 6.34244268216122e-08, 'fear': 50.114643573760986, 'happy': 4.883679002523422, 'sad': 14.459079504013062, 'surprise': 3.5078436136245728, 'neutral': 27.034109830856323}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0007392753587752852, 'disgust': 9.276648897857298e-08, 'fear': 42.95465136114939, 'happy': 7.5564800135517105, 'sad': 13.1204619264052, 'surprise': 5.309753232031675, 'neutral': 31.057911389123035}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0006220243449206464, 'disgust': 9.259706335029705e-08, 'fear': 37.735769152641296, 'happy': 5.884786322712898, 'sad': 18.922363221645355, 'surprise': 2.0606160163879395, 'neutral': 35.39584279060364}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0005231228897173423, 'disgust': 8.375478088851196e-08, 'fear': 34.04792249202728, 'happy': 4.046181961894035, 'sad': 21.07149362564087, 'surprise': 1.8449347466230392, 'neutral': 38.98894190788269}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0004440529210114619, 'disgust': 3.028569894869548e-08, 'fear': 47.700417041778564, 'happy': 4.520949348807335, 'sad': 13.042408227920532, 'surprise': 3.8942787796258926, 'neutral': 30.841505527496338}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0005987477281810526, 'disgust': 8.896971468055924e-08, 'fear': 28.35573723052366, 'happy': 5.349779820429076, 'sad': 25.062133375533424, 'surprise': 1.9056386518859059, 'neutral': 39.32610984600245}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0008676515790284611, 'disgust': 1.225684553851636e-07, 'fear': 37.59308159351349, 'happy': 6.655813008546829, 'sad': 13.554690778255463, 'surprise': 6.922829896211624, 'neutral': 35.272714495658875}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00048244633035210427, 'disgust': 4.6501075212646015e-08, 'fear': 47.49103784561157, 'happy': 4.779185727238655, 'sad': 14.284704625606537, 'surprise': 4.0823884308338165, 'neutral': 29.362201690673828}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0006275613031903049, 'disgust': 5.1460952166237917e-08, 'fear': 46.63171470165253, 'happy': 5.578182637691498, 'sad': 13.345783948898315, 'surprise': 3.414681926369667, 'neutral': 31.02900981903076}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00045216188482299913, 'disgust': 5.048099160909203e-08, 'fear': 43.997594714164734, 'happy': 4.126165434718132, 'sad': 16.932202875614166, 'surprise': 2.67579834908247, 'neutral': 32.26779103279114}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0005107687229610747, 'disgust': 3.4572134044452696e-08, 'fear': 43.86281073093414, 'happy': 5.674934014678001, 'sad': 13.915017247200012, 'surprise': 3.7057198584079742, 'neutral': 32.84101188182831}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0005148824436613139, 'disgust': 1.2563887346626686e-07, 'fear': 34.08121669776539, 'happy': 3.458650988856017, 'sad': 23.25651486256807, 'surprise': 1.5452202399206305, 'neutral': 37.6578830276658}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0007906555765657686, 'disgust': 1.7316834632907785e-07, 'fear': 32.22215175628662, 'happy': 6.684914976358414, 'sad': 15.812583267688751, 'surprise': 4.24114428460598, 'neutral': 41.03841483592987}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0006218729140528012, 'disgust': 1.0728615773558658e-07, 'fear': 36.36917769908905, 'happy': 5.068359896540642, 'sad': 18.26929748058319, 'surprise': 2.87297572940588, 'neutral': 37.41956949234009}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0008969548616732936, 'disgust': 2.1725141685635663e-07, 'fear': 35.821518301963806, 'happy': 8.001706749200821, 'sad': 15.74462503194809, 'surprise': 3.8482867181301117, 'neutral': 36.58297061920166}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0006061311069061048, 'disgust': 8.909752935437609e-08, 'fear': 38.15608024597168, 'happy': 5.132484063506126, 'sad': 17.142963409423828, 'surprise': 2.8729550540447235, 'neutral': 36.69491410255432}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0005706228421331616, 'disgust': 1.6176444628257514e-07, 'fear': 35.1718932390213, 'happy': 4.503319412469864, 'sad': 22.526952624320984, 'surprise': 1.7631744965910912, 'neutral': 36.03408932685852}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00048408172162888125, 'disgust': 1.978965165344441e-08, 'fear': 46.788766788766786, 'happy': 4.982975794850337, 'sad': 11.40276261584536, 'surprise': 4.329354201516759, 'neutral': 32.49565556619498}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.000681168553129339, 'disgust': 5.684508758357621e-08, 'fear': 39.33733638151505, 'happy': 7.637237467601149, 'sad': 16.410907591039393, 'surprise': 2.1420074040894153, 'neutral': 34.47182980011879}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0006308382126007928, 'disgust': 7.186733985697913e-08, 'fear': 41.169652342796326, 'happy': 3.568233922123909, 'sad': 19.176486134529114, 'surprise': 1.9685333594679832, 'neutral': 34.116461873054504}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0006319867679849267, 'disgust': 7.767581022832815e-08, 'fear': 44.49155628681183, 'happy': 4.957258328795433, 'sad': 14.568424224853516, 'surprise': 3.602803871035576, 'neutral': 32.379332184791565}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0007304540797648978, 'disgust': 6.137637998922128e-08, 'fear': 42.77497914135705, 'happy': 11.79348536807473, 'sad': 9.67280674900928, 'surprise': 6.402337532956894, 'neutral': 29.355656581442762}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.000599081158725312, 'disgust': 4.277550258446894e-08, 'fear': 53.889042139053345, 'happy': 5.690411478281021, 'sad': 9.99688133597374, 'surprise': 3.8382649421691895, 'neutral': 26.584801077842712}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0004584185170771599, 'disgust': 3.923511325516692e-08, 'fear': 42.646489423931904, 'happy': 6.284199650979084, 'sad': 12.84853364842729, 'surprise': 4.2110907481800615, 'neutral': 34.009223102423775}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0005059396550545694, 'disgust': 3.681329246685607e-08, 'fear': 51.028609153198104, 'happy': 7.05872168436984, 'sad': 11.70906657468479, 'surprise': 3.132503851353663, 'neutral': 27.070590606857465}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00046962049964349717, 'disgust': 4.18519996436828e-08, 'fear': 42.509809136390686, 'happy': 5.229656025767326, 'sad': 13.86033147573471, 'surprise': 3.988392651081085, 'neutral': 34.411343932151794}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0005932991724716471, 'disgust': 4.3889125507545186e-08, 'fear': 45.34198614013113, 'happy': 6.0777857797018155, 'sad': 13.502719313068349, 'surprise': 3.1737716897589974, 'neutral': 31.903143638559797}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00037986133065714967, 'disgust': 3.5596087188949355e-08, 'fear': 46.18488252162933, 'happy': 6.197657808661461, 'sad': 12.017648667097092, 'surprise': 4.017922282218933, 'neutral': 31.581515073776245}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0004634209566309035, 'disgust': 4.022206418974953e-08, 'fear': 55.11725277407484, 'happy': 4.839520072312359, 'sad': 10.665149132320233, 'surprise': 5.920734236880198, 'neutral': 23.45688035827162}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0005200933173910016, 'disgust': 7.501236853890703e-08, 'fear': 48.3386367559433, 'happy': 4.593387618660927, 'sad': 12.778501212596893, 'surprise': 3.5458799451589584, 'neutral': 30.74306845664978}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0006185945211833122, 'disgust': 6.964867296402964e-08, 'fear': 36.45879489674629, 'happy': 8.575790843273301, 'sad': 10.78134795649672, 'surprise': 7.6544789785767815, 'neutral': 36.52896147621137}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00047228527364495676, 'disgust': 4.619919724557775e-08, 'fear': 48.10543358325958, 'happy': 5.282885581254959, 'sad': 14.690741896629333, 'surprise': 2.400887943804264, 'neutral': 29.51958477497101}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.06686272099614143, 'disgust': 1.1068816974102447e-05, 'fear': 0.10678464313969016, 'happy': 1.2828763574361801, 'sad': 2.9909975826740265, 'surprise': 0.002293087891303003, 'neutral': 95.55017948150635}, 'dominant_emotion': 'neutral', 'region': {'x': 911, 'y': 258, 'w': 237, 'h': 237}, 'face_confidence': 2.7197459452436306}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.010261669260679588, 'disgust': 8.483422470562414e-06, 'fear': 0.0061424656474197295, 'happy': 83.35982461928276, 'sad': 0.9492656781831788, 'surprise': 0.004306216835368311, 'neutral': 15.670194665801207}, 'dominant_emotion': 'happy', 'region': {'x': 888, 'y': 263, 'w': 227, 'h': 227}, 'face_confidence': 3.660446104768198}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.056363490875810385, 'disgust': 5.846408157594851e-05, 'fear': 0.04576397477649152, 'happy': 49.29569065570831, 'sad': 2.420259825885296, 'surprise': 0.00643552266410552, 'neutral': 48.175427317619324}, 'dominant_emotion': 'happy', 'region': {'x': 877, 'y': 260, 'w': 232, 'h': 232}, 'face_confidence': 4.369582055311184}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.170615641400218, 'disgust': 3.815031845988415e-05, 'fear': 0.08306859526783228, 'happy': 28.325361013412476, 'sad': 1.4081060886383057, 'surprise': 0.05336703616194427, 'neutral': 69.9594497680664}, 'dominant_emotion': 'neutral', 'region': {'x': 872, 'y': 258, 'w': 237, 'h': 237}, 'face_confidence': 4.514020092028659}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.02879507082980126, 'disgust': 5.3160778179517365e-05, 'fear': 0.03975705767516047, 'happy': 51.46327614784241, 'sad': 3.7987306714057922, 'surprise': 0.004592747427523136, 'neutral': 44.66479420661926}, 'dominant_emotion': 'happy', 'region': {'x': 866, 'y': 258, 'w': 233, 'h': 233}, 'face_confidence': 4.536229091871064}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.015350656758528203, 'disgust': 1.2346066569079994e-05, 'fear': 0.018185754015576094, 'happy': 51.37268304824829, 'sad': 2.2951528429985046, 'surprise': 0.002247366683150176, 'neutral': 46.29637002944946}, 'dominant_emotion': 'happy', 'region': {'x': 863, 'y': 261, 'w': 229, 'h': 229}, 'face_confidence': 5.774063075135928}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0006489353381766705, 'disgust': 4.536530262555516e-07, 'fear': 9.580189725966193e-05, 'happy': 97.55411148071289, 'sad': 0.1351074897684157, 'surprise': 0.0009636741197027732, 'neutral': 2.3090723901987076}, 'dominant_emotion': 'happy', 'region': {'x': 857, 'y': 266, 'w': 224, 'h': 224}, 'face_confidence': 4.278430011298042}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00011340585609301906, 'disgust': 2.3058250018171267e-06, 'fear': 0.00019147381640180177, 'happy': 93.61603818035353, 'sad': 0.07440243359818659, 'surprise': 8.684868879910543e-05, 'neutral': 6.3091572409365915}, 'dominant_emotion': 'happy', 'region': {'x': 823, 'y': 264, 'w': 233, 'h': 233}, 'face_confidence': 3.177590077917557}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 4.5105594814742744e-07, 'disgust': 2.1114202053418296e-08, 'fear': 0.0005554947620112216, 'happy': 5.5313410030066734e-05, 'sad': 99.98034834861755, 'surprise': 1.6159265592285976e-08, 'neutral': 0.019042905478272587}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 9.220028118761547e-06, 'disgust': 3.017516903014439e-07, 'fear': 1.3776846685686905e-05, 'happy': 96.5414047241211, 'sad': 0.0004906775302515598, 'surprise': 0.0001614897769286472, 'neutral': 3.457919880747795}, 'dominant_emotion': 'happy', 'region': {'x': 725, 'y': 273, 'w': 235, 'h': 235}, 'face_confidence': 2.484819157689344}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00026566146971163107, 'disgust': 2.3885519340183237e-06, 'fear': 0.0013742429473495577, 'happy': 0.014403501700144261, 'sad': 99.95405673980713, 'surprise': 9.507126197405569e-08, 'neutral': 0.02989773638546467}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0011360355529177468, 'disgust': 2.9977169901940215e-05, 'fear': 0.008802986121736467, 'happy': 0.021567555086221546, 'sad': 98.23092818260193, 'surprise': 1.7797601614688574e-06, 'neutral': 1.7375370487570763}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.005930467149172814, 'disgust': 0.00018406952864386824, 'fear': 0.022684382462688234, 'happy': 0.2639846102288133, 'sad': 81.43428453411367, 'surprise': 0.00024900474847415824, 'neutral': 18.272679643194653}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00013813298037836466, 'disgust': 8.373659209487195e-07, 'fear': 1.2905053498790195e-05, 'happy': 99.7792005275727, 'sad': 0.0010314084967729055, 'surprise': 6.052553811350966e-05, 'neutral': 0.21955997506498992}, 'dominant_emotion': 'happy', 'region': {'x': 642, 'y': 260, 'w': 264, 'h': 264}, 'face_confidence': 3.9715061096358113}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.027233495739393446, 'disgust': 0.0004890583894831605, 'fear': 0.0809535578712305, 'happy': 99.47744018108365, 'sad': 0.17871092038694972, 'surprise': 0.03632800511551587, 'neutral': 0.19884407478194538}, 'dominant_emotion': 'happy', 'region': {'x': 662, 'y': 255, 'w': 250, 'h': 250}, 'face_confidence': 2.7890931500005536}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 5.968537554144859, 'disgust': 0.006220152863534167, 'fear': 9.909048676490784, 'happy': 66.89550280570984, 'sad': 3.581421449780464, 'surprise': 3.0398577451705933, 'neutral': 10.59941053390503}, 'dominant_emotion': 'happy', 'region': {'x': 655, 'y': 249, 'w': 268, 'h': 268}, 'face_confidence': 4.9111220766208135}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 5.882089212536812, 'disgust': 0.31957209575921297, 'fear': 17.77971237897873, 'happy': 0.7141288835555315, 'sad': 36.9007408618927, 'surprise': 0.02480452531017363, 'neutral': 38.37894797325134}, 'dominant_emotion': 'neutral', 'region': {'x': 666, 'y': 247, 'w': 263, 'h': 263}, 'face_confidence': 5.4691790551296435}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.026136220549233258, 'disgust': 4.2625939045137784e-05, 'fear': 0.14860481023788452, 'happy': 54.951053857803345, 'sad': 3.951973095536232, 'surprise': 0.01870297419372946, 'neutral': 40.90348482131958}, 'dominant_emotion': 'happy', 'region': {'x': 675, 'y': 245, 'w': 263, 'h': 263}, 'face_confidence': 4.553486100805458}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.010741822916315868, 'disgust': 7.453472505858372e-06, 'fear': 0.14403918758034706, 'happy': 14.894607663154602, 'sad': 4.396802559494972, 'surprise': 0.0039047226891852915, 'neutral': 80.54989576339722}, 'dominant_emotion': 'neutral', 'region': {'x': 671, 'y': 238, 'w': 272, 'h': 272}, 'face_confidence': 2.951061090629082}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.015752995386719704, 'disgust': 1.2884819966529903e-05, 'fear': 0.17083969432860613, 'happy': 15.929657220840454, 'sad': 2.8876418247818947, 'surprise': 0.00986803715932183, 'neutral': 80.98623156547546}, 'dominant_emotion': 'neutral', 'region': {'x': 672, 'y': 240, 'w': 270, 'h': 270}, 'face_confidence': 8.40983904938912}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0023789767510606907, 'disgust': 1.944184147006922e-07, 'fear': 0.003145602750009857, 'happy': 1.138603687286377, 'sad': 22.28655070066452, 'surprise': 0.0021400310288299806, 'neutral': 76.56718492507935}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00026857853754336247, 'disgust': 4.751083901299857e-09, 'fear': 0.005212087853578851, 'happy': 0.008669048838783056, 'sad': 94.49283480644226, 'surprise': 0.00012647949461097596, 'neutral': 5.4928891360759735}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0011053060916310642, 'disgust': 8.541435647657636e-06, 'fear': 2.1170420572161674, 'happy': 0.06640706560574472, 'sad': 95.52605152130127, 'surprise': 3.362023903719091e-05, 'neutral': 2.2893527522683144}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 9.665080114018565e-06, 'disgust': 2.9957341052266884e-07, 'fear': 0.5740349646657705, 'happy': 0.026554902433417737, 'sad': 96.01210355758667, 'surprise': 0.00016294168290187372, 'neutral': 3.387138992547989}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 1.3579675537507894e-05, 'disgust': 1.1739105310578463e-06, 'fear': 0.7364274880501613, 'happy': 0.06661963046278124, 'sad': 94.51945358746691, 'surprise': 0.00018857351707693673, 'neutral': 4.677298850095135}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0018550217646406963, 'disgust': 0.00020950853922840906, 'fear': 5.009160190820694, 'happy': 0.09480118751525879, 'sad': 90.72842001914978, 'surprise': 0.00040693694245419465, 'neutral': 4.165147989988327}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0019371065228365968, 'disgust': 0.001011107331098301, 'fear': 3.014076886883153, 'happy': 0.22131674613007948, 'sad': 87.52738386066153, 'surprise': 0.0009631790578661771, 'neutral': 9.233301522338209}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.002497512468835339, 'disgust': 0.0017575412130099721, 'fear': 2.789837308228016, 'happy': 0.182674964889884, 'sad': 89.10214900970459, 'surprise': 0.0007323056706809439, 'neutral': 7.92035311460495}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0017845131570541448, 'disgust': 0.0009634607283417385, 'fear': 2.233895579410126, 'happy': 0.21319079465379778, 'sad': 90.07018326876363, 'surprise': 0.0005275654810711214, 'neutral': 7.4794507051169035}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0025313188705559033, 'disgust': 0.0012763703737586345, 'fear': 2.3893914239873197, 'happy': 0.49547540342514557, 'sad': 86.94305860317452, 'surprise': 0.0008180027245700813, 'neutral': 10.167453159982342}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.003333156894874682, 'disgust': 0.0021056789810285814, 'fear': 3.1831592252064675, 'happy': 0.507096885788812, 'sad': 84.27609988735915, 'surprise': 0.0012439807841919453, 'neutral': 12.026953008538126}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0012710552103270623, 'disgust': 0.001054884041089172, 'fear': 2.5160602862274817, 'happy': 0.22752426058198574, 'sad': 88.15253306344349, 'surprise': 0.0005555668271059654, 'neutral': 9.10100320583601}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0035193748772144318, 'disgust': 0.002725687409110833, 'fear': 3.8103558123111725, 'happy': 0.596198532730341, 'sad': 82.82431364059448, 'surprise': 0.0010327698873879854, 'neutral': 12.761853635311127}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.006252517778193578, 'disgust': 0.0015001033716544043, 'fear': 6.154429912567139, 'happy': 0.5598177202045918, 'sad': 78.32683324813843, 'surprise': 0.0028193273465149105, 'neutral': 14.948347210884094}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0024877695977920666, 'disgust': 0.001680335662967991, 'fear': 1.2666873633861542, 'happy': 0.08532655192539096, 'sad': 96.87109589576721, 'surprise': 5.279093784338329e-05, 'neutral': 1.772666908800602}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.01652806625646308, 'disgust': 0.0009414395905001856, 'fear': 29.80015816686652, 'happy': 0.4728335323532185, 'sad': 63.243769020584935, 'surprise': 0.003637534176251502, 'neutral': 6.462127690061606}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0008045564754866064, 'disgust': 6.234366622948073e-06, 'fear': 64.61123824119568, 'happy': 0.439938111230731, 'sad': 20.827457308769226, 'surprise': 0.005472512930282392, 'neutral': 14.115084707736969}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.007600518891292819, 'disgust': 7.462945906437765e-06, 'fear': 92.17755747899756, 'happy': 0.5921007021278561, 'sad': 4.214342189690005, 'surprise': 0.17871673818479408, 'neutral': 2.829679642896631}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.003062877841083502, 'disgust': 3.6220522075607853e-06, 'fear': 88.91456657138863, 'happy': 0.33268413626755095, 'sad': 6.649736711367173, 'surprise': 0.014435877661589692, 'neutral': 4.08551270279364}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0020291984892800386, 'disgust': 2.105994594126584e-06, 'fear': 83.47562748484283, 'happy': 0.9549065286032523, 'sad': 6.817160479168835, 'surprise': 0.06174711007286786, 'neutral': 8.688522584614446}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0016958681953838095, 'disgust': 1.0147299001062038e-06, 'fear': 81.14721775054932, 'happy': 1.5395312570035458, 'sad': 3.491850197315216, 'surprise': 0.2714599249884486, 'neutral': 13.54825347661972}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.002214412151945517, 'disgust': 7.157002137073428e-07, 'fear': 83.70178536155399, 'happy': 1.999909751127988, 'sad': 2.1463618849084516, 'surprise': 0.7993619867429749, 'neutral': 11.35035990472318}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0013278290916412382, 'disgust': 4.444331039625369e-07, 'fear': 77.91496182501771, 'happy': 1.5218535486634315, 'sad': 2.2120877326026283, 'surprise': 0.9069843999166012, 'neutral': 17.442783124115095}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0020268222215236165, 'disgust': 1.125934279144758e-06, 'fear': 66.71107411384583, 'happy': 1.9407911226153374, 'sad': 3.5535290837287903, 'surprise': 1.0960837826132774, 'neutral': 26.696503162384033}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.005263541606836952, 'disgust': 2.8001375440567244e-06, 'fear': 63.05850148200989, 'happy': 2.4202901870012283, 'sad': 2.2711437195539474, 'surprise': 2.721989154815674, 'neutral': 29.522809386253357}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.002116551402123719, 'disgust': 9.735837297883486e-07, 'fear': 65.87297509586514, 'happy': 1.9479766947599286, 'sad': 3.4280844394069243, 'surprise': 1.0580622921064249, 'neutral': 27.690792404507686}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.001956188796794585, 'disgust': 9.325869521824356e-07, 'fear': 60.8491874247305, 'happy': 2.8316315088648505, 'sad': 4.3257965490696755, 'surprise': 0.83274185033094, 'neutral': 31.158687541406604}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0016393609257647768, 'disgust': 9.501243347642685e-07, 'fear': 65.279620885849, 'happy': 1.9134977832436562, 'sad': 4.330480098724365, 'surprise': 0.7944729179143906, 'neutral': 27.680286765098572}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0019109214917989448, 'disgust': 5.968312954962585e-07, 'fear': 66.65353178977966, 'happy': 2.53417007625103, 'sad': 2.2643208503723145, 'surprise': 1.8664052709937096, 'neutral': 26.679661870002747}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0014930165889381897, 'disgust': 6.289785581259366e-07, 'fear': 67.31514930725098, 'happy': 1.3245525769889355, 'sad': 2.0780643448233604, 'surprise': 2.9462914913892746, 'neutral': 26.33444368839264}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0011303305419254161, 'disgust': 6.363285833395412e-07, 'fear': 70.90248888149792, 'happy': 1.5581768003509522, 'sad': 2.885510013133884, 'surprise': 0.8312654245654002, 'neutral': 23.821425367678724}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0011757327229133807, 'disgust': 6.660603624197847e-07, 'fear': 70.75398564338684, 'happy': 1.438627950847149, 'sad': 3.2661519944667816, 'surprise': 0.7151976693421602, 'neutral': 23.824866116046906}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00206500590138603, 'disgust': 1.309476171940105e-06, 'fear': 69.3388819694519, 'happy': 1.9169103354215622, 'sad': 3.9739668369293213, 'surprise': 0.8614134974777699, 'neutral': 23.906761407852173}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0016596992281847633, 'disgust': 9.821007118659963e-07, 'fear': 57.81109929084778, 'happy': 2.0422128960490227, 'sad': 3.6998201161623, 'surprise': 0.8127434179186821, 'neutral': 35.63246130943298}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0007894802867832734, 'disgust': 4.275103702770674e-07, 'fear': 80.00440836031586, 'happy': 0.9360998680174115, 'sad': 2.693673704923458, 'surprise': 0.4539252139196182, 'neutral': 15.91110663016557}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.004498676035245385, 'disgust': 1.7491167316641364e-06, 'fear': 90.946425086686, 'happy': 1.1623896204066728, 'sad': 2.63260906844031, 'surprise': 0.31975608698206104, 'neutral': 4.93431926556596}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0006064794433768839, 'disgust': 6.645197725418939e-07, 'fear': 78.86043190956116, 'happy': 0.2740959171205759, 'sad': 14.811620116233826, 'surprise': 0.013180691166780889, 'neutral': 6.040070578455925}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0013160194612282794, 'disgust': 2.1308119713125961e-07, 'fear': 27.908793091773987, 'happy': 1.098152995109558, 'sad': 60.01461148262024, 'surprise': 0.018857592658605427, 'neutral': 10.958264768123627}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0014968591113739957, 'disgust': 2.0751268607982765e-07, 'fear': 34.200297844427695, 'happy': 2.19461167273591, 'sad': 44.36477389125668, 'surprise': 0.04687143070602004, 'neutral': 19.191950213429344}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.001605985658215901, 'disgust': 1.7184188430629067e-07, 'fear': 43.436202015650395, 'happy': 1.9416961113629407, 'sad': 37.53162250111237, 'surprise': 0.03360034516120524, 'neutral': 17.055275562720034}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0007147616997826844, 'disgust': 1.3237045903835565e-07, 'fear': 32.82122611999512, 'happy': 1.3612435199320316, 'sad': 41.04858636856079, 'surprise': 0.05455988575704396, 'neutral': 24.71366822719574}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0009154454528372902, 'disgust': 1.3578240152192705e-07, 'fear': 35.49120551242894, 'happy': 2.7517165543695157, 'sad': 37.96559428758553, 'surprise': 0.05345578011343242, 'neutral': 23.73711153711218}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0006583846698049456, 'disgust': 1.4239319723330368e-07, 'fear': 28.842660784721375, 'happy': 1.104544010013342, 'sad': 49.0032821893692, 'surprise': 0.010946530528599396, 'neutral': 21.037903428077698}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0014310355254565366, 'disgust': 4.808676568046621e-07, 'fear': 24.38218742609024, 'happy': 1.8424365669488907, 'sad': 57.65044689178467, 'surprise': 0.05875719944015145, 'neutral': 16.06474220752716}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00048377251005149446, 'disgust': 6.341396852072023e-08, 'fear': 18.375122547149658, 'happy': 0.5944120232015848, 'sad': 71.58172130584717, 'surprise': 0.0017063222912838683, 'neutral': 9.446554630994797}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 7.247599273835174e-05, 'disgust': 1.6563991257995104e-08, 'fear': 15.109789060379379, 'happy': 0.12090351388263537, 'sad': 45.17505226432654, 'surprise': 0.00928359621246316, 'neutral': 39.58489124955043}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 2.5685233140393393e-05, 'disgust': 1.3040850344037125e-08, 'fear': 11.37404665350914, 'happy': 0.021520836162380874, 'sad': 46.630269289016724, 'surprise': 0.0007930830179248005, 'neutral': 41.9733464717865}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.17739074537530541, 'disgust': 3.420836947043426e-05, 'fear': 0.2079741796478629, 'happy': 0.732447998598218, 'sad': 2.8672702610492706, 'surprise': 0.0005234983746049693, 'neutral': 96.01435661315918}, 'dominant_emotion': 'neutral', 'region': {'x': 873, 'y': 253, 'w': 239, 'h': 239}, 'face_confidence': 4.211721040424891}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 2.623245336508262e-05, 'disgust': 7.515246064349057e-09, 'fear': 10.72842925786972, 'happy': 0.09119795868173242, 'sad': 9.983275830745697, 'surprise': 0.025791459484025836, 'neutral': 79.17128205299377}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 4.877581432083389e-05, 'disgust': 1.3995461733973258e-08, 'fear': 16.06045961380005, 'happy': 0.12454765383154154, 'sad': 13.631507754325867, 'surprise': 0.05334999295882881, 'neutral': 70.13009190559387}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 3.1129565059018205e-05, 'disgust': 4.575849421595279e-09, 'fear': 14.694099128246307, 'happy': 0.18000896088778973, 'sad': 5.438752472400665, 'surprise': 0.193226698320359, 'neutral': 79.4938862323761}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 2.5563841177245195e-05, 'disgust': 2.5844512294148636e-09, 'fear': 19.205676019191742, 'happy': 0.08616273989900947, 'sad': 4.004013538360596, 'surprise': 0.12977414298802614, 'neutral': 76.57434940338135}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 1.3402564036368858e-05, 'disgust': 1.6922322917944932e-09, 'fear': 17.030374705791473, 'happy': 0.02921380801126361, 'sad': 18.373891711235046, 'surprise': 0.008931316551752388, 'neutral': 64.55757021903992}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 9.839772729947072e-06, 'disgust': 7.840477399267964e-10, 'fear': 14.162279665470123, 'happy': 0.11846066918224096, 'sad': 3.1071389093995094, 'surprise': 0.097852130420506, 'neutral': 82.51425623893738}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 1.2456710862807086e-05, 'disgust': 8.522717254155765e-10, 'fear': 20.795610547065735, 'happy': 0.09122638730332255, 'sad': 4.259571060538292, 'surprise': 0.11223402107134461, 'neutral': 74.74135160446167}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 2.3943142941004967e-05, 'disgust': 3.251974924999373e-09, 'fear': 16.39777072654788, 'happy': 0.17110889167615126, 'sad': 5.078062345270058, 'surprise': 0.14113218899114066, 'neutral': 78.2119022734107}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 4.163813933136871e-05, 'disgust': 5.947193786645694e-09, 'fear': 16.25053085389917, 'happy': 0.4008775815890778, 'sad': 2.2357548988315403, 'surprise': 0.6385639224835588, 'neutral': 80.4742324634929}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 4.446384593848052e-05, 'disgust': 5.354153231884595e-09, 'fear': 20.57536244392395, 'happy': 0.4878799431025982, 'sad': 2.1535780280828476, 'surprise': 0.8232244290411472, 'neutral': 75.95990896224976}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 3.057834589981212e-05, 'disgust': 4.349441640183471e-09, 'fear': 16.41160249710083, 'happy': 0.2910755341872573, 'sad': 2.909698151051998, 'surprise': 0.33726717811077833, 'neutral': 80.05032539367676}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 5.783148822047224e-05, 'disgust': 1.0249394388761601e-08, 'fear': 24.52908754348755, 'happy': 0.4281753674149513, 'sad': 2.4011196568608284, 'surprise': 1.0188452899456024, 'neutral': 71.6227114200592}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 7.519664109167934e-05, 'disgust': 1.9074136714536394e-08, 'fear': 26.17700695991516, 'happy': 0.5381457973271608, 'sad': 2.6570552960038185, 'surprise': 0.6553378887474537, 'neutral': 69.97237801551819}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 8.411330441049358e-05, 'disgust': 2.031545626168807e-08, 'fear': 20.621852576732635, 'happy': 0.6157823838293552, 'sad': 2.302740700542927, 'surprise': 0.995964277535677, 'neutral': 75.4635751247406}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 6.644468726335617e-05, 'disgust': 1.1789506870751865e-08, 'fear': 23.835065960884094, 'happy': 0.26284134946763515, 'sad': 4.033213108778, 'surprise': 0.3239547135308385, 'neutral': 71.54485583305359}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0001648632958717653, 'disgust': 2.542168294185766e-08, 'fear': 27.07053286257582, 'happy': 0.1988734470008878, 'sad': 46.096193557750794, 'surprise': 0.010406398980188681, 'neutral': 26.62382880591326}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00026998272915079724, 'disgust': 3.5757036220829264e-08, 'fear': 33.90188813209534, 'happy': 0.21023922599852085, 'sad': 44.5724219083786, 'surprise': 0.022554492170456797, 'neutral': 21.29262536764145}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00018699663519783962, 'disgust': 1.9854495620841424e-08, 'fear': 31.043915810818422, 'happy': 0.3429293836909165, 'sad': 18.07061094466513, 'surprise': 0.21696552996131957, 'neutral': 50.32539071592037}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 6.572323627551668e-05, 'disgust': 1.0124768384800475e-08, 'fear': 17.84481853246689, 'happy': 0.07256609969772398, 'sad': 54.9394428730011, 'surprise': 0.0042205476347589865, 'neutral': 27.138888835906982}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 8.942081492881239e-05, 'disgust': 1.0628283790611001e-08, 'fear': 22.268347767788438, 'happy': 0.08048779768642214, 'sad': 47.62252597540307, 'surprise': 0.009845910526088861, 'neutral': 30.018698570883444}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0001208512912853621, 'disgust': 3.4527380954330056e-08, 'fear': 24.221843481063843, 'happy': 0.31337118707597256, 'sad': 32.153233885765076, 'surprise': 0.0401944707846269, 'neutral': 43.27123463153839}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00011631079814833356, 'disgust': 1.3765524831121922e-08, 'fear': 26.216477155685425, 'happy': 0.20587709732353687, 'sad': 31.3403457403183, 'surprise': 0.037092846469022334, 'neutral': 42.20009744167328}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 9.185623639496043e-05, 'disgust': 2.057653492038014e-08, 'fear': 18.224337697029114, 'happy': 0.1422413159161806, 'sad': 45.092037320137024, 'surprise': 0.01927602424984798, 'neutral': 36.52201592922211}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.15944632468745112, 'disgust': 2.5237872902650338e-08, 'fear': 0.0073733732278924435, 'happy': 0.01859022886492312, 'sad': 0.23881029337644577, 'surprise': 5.389975399339164e-05, 'neutral': 99.57572817802429}, 'dominant_emotion': 'neutral', 'region': {'x': 931, 'y': 255, 'w': 241, 'h': 241}, 'face_confidence': 4.331635061593261}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00012665551967165107, 'disgust': 1.5449786349051227e-08, 'fear': 22.31649128395215, 'happy': 0.19432693940945736, 'sad': 45.81419179608724, 'surprise': 0.015367014943135821, 'neutral': 31.659499604642345}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.14454236952587962, 'disgust': 4.102699568964141e-08, 'fear': 0.012580408656504005, 'happy': 0.010352112440159544, 'sad': 0.32909505534917116, 'surprise': 0.00014206706282493542, 'neutral': 99.50329065322876}, 'dominant_emotion': 'neutral', 'region': {'x': 939, 'y': 260, 'w': 223, 'h': 223}, 'face_confidence': 5.447740004747175}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0001728662596178765, 'disgust': 2.5876231712906872e-08, 'fear': 26.10158920288086, 'happy': 0.30345674604177475, 'sad': 46.75237238407135, 'surprise': 0.015840199193917215, 'neutral': 26.826566457748413}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 8.785389127297094e-05, 'disgust': 1.007609898295847e-08, 'fear': 19.11786049604416, 'happy': 0.17908875597640872, 'sad': 47.824594378471375, 'surprise': 0.012006004544673488, 'neutral': 32.866370677948}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00011883961406056187, 'disgust': 1.6053317009045998e-08, 'fear': 20.30181735754013, 'happy': 0.6154533009976149, 'sad': 18.882156908512115, 'surprise': 0.151469511911273, 'neutral': 60.04898548126221}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 7.543790161435027e-05, 'disgust': 8.580617466558138e-09, 'fear': 23.17395657300949, 'happy': 0.179253367241472, 'sad': 39.595210552215576, 'surprise': 0.014858352369628847, 'neutral': 37.03664541244507}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00010830899063706784, 'disgust': 2.3880265855937846e-08, 'fear': 19.353970548747213, 'happy': 1.150957097915238, 'sad': 12.863377503357977, 'surprise': 0.1720293760391698, 'neutral': 66.45955839512101}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.0001741627443152538, 'disgust': 3.465463471741259e-08, 'fear': 24.987879395484924, 'happy': 0.7764169946312904, 'sad': 17.822647094726562, 'surprise': 0.14929393073543906, 'neutral': 56.26358389854431}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00011920548104171758, 'disgust': 3.4106667490263476e-08, 'fear': 16.977226734161377, 'happy': 0.6836372427642345, 'sad': 21.930991113185883, 'surprise': 0.08679329766891897, 'neutral': 60.321229696273804}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.001946138945640996, 'disgust': 2.6251745569028095e-09, 'fear': 0.004744750549434684, 'happy': 0.0803458271548152, 'sad': 0.08161296136677265, 'surprise': 0.0005585973667621147, 'neutral': 99.83079433441162}, 'dominant_emotion': 'neutral', 'region': {'x': 871, 'y': 241, 'w': 228, 'h': 228}, 'face_confidence': 3.112886149145197}]\n",
      "list indices must be integers or slices, not str\n",
      "[{'emotion': {'angry': 0.00012546499485549602, 'disgust': 4.582653141437595e-08, 'fear': 8.582448725846334, 'happy': 0.6254854519209535, 'sad': 13.000126957900939, 'surprise': 0.24157966530052813, 'neutral': 77.55023703278523}, 'dominant_emotion': 'neutral', 'region': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}, 'face_confidence': 0}]\n",
      "list indices must be integers or slices, not str\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    if not ret:\n",
    "        break  # If there's an error or no frame, exit the loop\n",
    "\n",
    "    # Analyze the frame with DeepFace\n",
    "    try:\n",
    "        # This will analyze the facial attributes such as emotion, age, gender, and race\n",
    "        # You can reduce the actions to speed up the process if needed\n",
    "        analysis = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
    "        print(analysis)\n",
    "\n",
    "        # Use the information in 'analysis' to draw boxes or text on the frame\n",
    "        # For instance, here's how you might display the emotion:\n",
    "        emotion = analysis[\"dominant_emotion\"]\n",
    "        cv2.putText(frame, emotion, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2, cv2.LINE_AA)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)  # Print any errors\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything's done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d19fa7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verified': False, 'distance': 0.5289615420480848, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 5.51}\n",
      "{'verified': False, 'distance': 0.541817496754095, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 5.02}\n",
      "{'verified': False, 'distance': 0.5199414249586043, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.67}\n",
      "{'verified': False, 'distance': 0.5220852285217006, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.38}\n",
      "{'verified': False, 'distance': 0.5272177331543957, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.58}\n",
      "{'verified': False, 'distance': 0.5474033605506649, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 5.11}\n",
      "{'verified': True, 'distance': 0.31043994834096356, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 609, 'y': 278, 'w': 266, 'h': 266}}, 'time': 4.6}\n",
      "img2_region not found in the result.\n",
      "{'verified': False, 'distance': 0.5379537672167125, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.13}\n",
      "{'verified': False, 'distance': 0.5376414881717244, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.99}\n",
      "{'verified': False, 'distance': 0.5356204431481401, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.71}\n",
      "{'verified': True, 'distance': 0.3751607401231781, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 594, 'y': 273, 'w': 253, 'h': 253}}, 'time': 4.79}\n",
      "img2_region not found in the result.\n",
      "{'verified': False, 'distance': 0.5265139177546687, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.73}\n",
      "{'verified': False, 'distance': 0.5306350441909891, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.68}\n",
      "{'verified': False, 'distance': 0.5310330620037224, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.61}\n",
      "{'verified': False, 'distance': 0.5221098028631701, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.81}\n",
      "{'verified': False, 'distance': 0.524721818450737, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.63}\n",
      "{'verified': False, 'distance': 0.5234068478629366, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.97}\n",
      "{'verified': True, 'distance': 0.35241441298034504, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 577, 'y': 228, 'w': 302, 'h': 302}}, 'time': 4.66}\n",
      "img2_region not found in the result.\n",
      "{'verified': True, 'distance': 0.2986007698461265, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 581, 'y': 227, 'w': 296, 'h': 296}}, 'time': 4.74}\n",
      "img2_region not found in the result.\n",
      "{'verified': True, 'distance': 0.2966111792608622, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 575, 'y': 227, 'w': 297, 'h': 297}}, 'time': 4.76}\n",
      "img2_region not found in the result.\n",
      "{'verified': False, 'distance': 0.5437962027171448, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.49}\n",
      "{'verified': False, 'distance': 0.5314909080395752, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.73}\n",
      "{'verified': False, 'distance': 0.5351536895834782, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 5.54}\n",
      "{'verified': False, 'distance': 0.5301546151722761, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 6.18}\n"
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "import cv2\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    if not ret:\n",
    "        break  # If there's an error or no frame, exit the loop\n",
    "\n",
    "    # Save the captured frame to a file\n",
    "    frame_path = 'current_frame.jpg'\n",
    "    cv2.imwrite(frame_path, frame)\n",
    "\n",
    "    try:\n",
    "        # Perform real-time face verification\n",
    "        result = DeepFace.verify(img1_path='./Downloads/profile_PIX_.JPG', img2_path=frame_path, enforce_detection=False)\n",
    "\n",
    "        # Print the result to understand its structure\n",
    "        print(result)\n",
    "\n",
    "        # Proceed only if verification was successful and the required data exists\n",
    "        if result[\"verified\"]:\n",
    "            # Check if 'img2_region' exists in the result before accessing it\n",
    "            if \"img2\" in result:\n",
    "                # Draw a rectangle around the face and tag as 'Found'\n",
    "                face_coordinates = result[\"img2\"]  # Coordinates of the face in the frame\n",
    "                cv2.rectangle(frame, (face_coordinates[0], face_coordinates[1]), (face_coordinates[2], face_coordinates[3]), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, \"Thats me!!\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            else:\n",
    "                print(\"img2_region not found in the result.\")\n",
    "        else:\n",
    "            cv2.putText(frame, \"Not Found\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)  # Print any errors\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything's done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5356b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'verified': True, 'distance': 0.1607575105175798, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 716, 'y': 238, 'w': 306, 'h': 306}}, 'time': 4.89}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "732f6b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verified': False, 'distance': 0.5705734559850948, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 183, 'h': 183}}, 'time': 5.26}\n",
      "{'verified': False, 'distance': 0.6346981148054035, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 212, 'h': 212}}, 'time': 6.97}\n",
      "{'verified': False, 'distance': 0.5142694949828077, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 252, 'h': 252}}, 'time': 5.36}\n",
      "{'verified': False, 'distance': 0.47788070128750904, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 251, 'h': 251}}, 'time': 4.64}\n",
      "{'verified': False, 'distance': 0.5141682349914104, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 253, 'h': 253}}, 'time': 5.0}\n",
      "{'verified': False, 'distance': 0.5501129279701791, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 253, 'h': 253}}, 'time': 5.06}\n",
      "{'verified': False, 'distance': 0.4621314004991819, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 247, 'h': 247}}, 'time': 5.03}\n",
      "{'verified': False, 'distance': 0.5065869875683879, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 248, 'h': 248}}, 'time': 4.87}\n",
      "{'verified': False, 'distance': 0.5067859945926074, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 346, 'h': 346}}, 'time': 4.66}\n",
      "{'verified': False, 'distance': 0.6284072460995281, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 179, 'h': 179}}, 'time': 4.87}\n",
      "{'verified': False, 'distance': 0.40162712033171455, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 181, 'h': 181}}, 'time': 5.09}\n",
      "{'verified': False, 'distance': 0.6711466448051506, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 55, 'h': 55}}, 'time': 7.09}\n",
      "{'verified': True, 'distance': 0.2778427729848051, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 189, 'h': 189}}, 'time': 6.21}\n",
      "{'verified': True, 'distance': 0.2848795005913861, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 206, 'h': 206}}, 'time': 6.2}\n",
      "{'verified': False, 'distance': 0.7101615640358276, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 54, 'h': 54}}, 'time': 5.08}\n",
      "{'verified': True, 'distance': 0.36478929835236995, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 182, 'h': 182}}, 'time': 4.56}\n",
      "{'verified': True, 'distance': 0.39415977958376713, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 10, 'y': 9, 'w': 155, 'h': 155}}, 'time': 4.59}\n",
      "{'verified': True, 'distance': 0.24189273639171527, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 199, 'h': 199}}, 'time': 5.18}\n",
      "{'verified': False, 'distance': 0.6889019728211565, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 53, 'h': 53}}, 'time': 7.56}\n",
      "{'verified': False, 'distance': 0.7019293170905625, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 54, 'h': 54}}, 'time': 6.17}\n",
      "{'verified': False, 'distance': 0.6822533039917087, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 54, 'h': 54}}, 'time': 7.59}\n",
      "{'verified': False, 'distance': 0.7171565466670713, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 54, 'h': 54}}, 'time': 5.67}\n",
      "{'verified': False, 'distance': 0.4403563046179322, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 231, 'h': 231}}, 'time': 4.9}\n",
      "{'verified': True, 'distance': 0.30340718250581444, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 230, 'h': 230}}, 'time': 4.77}\n",
      "{'verified': True, 'distance': 0.23367044755427502, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 231, 'h': 231}}, 'time': 4.66}\n",
      "{'verified': False, 'distance': 0.6880630355117847, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 54, 'h': 54}}, 'time': 4.77}\n",
      "{'verified': True, 'distance': 0.3756841462773144, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 204, 'h': 204}}, 'time': 4.87}\n",
      "{'verified': False, 'distance': 0.47536984133507965, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 157, 'h': 157}}, 'time': 4.64}\n",
      "{'verified': False, 'distance': 0.5052454285378153, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 193, 'h': 193}}, 'time': 4.53}\n",
      "{'verified': False, 'distance': 0.5087822944677245, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 175, 'h': 175}}, 'time': 4.63}\n",
      "{'verified': False, 'distance': 0.6708746817533566, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 55, 'h': 55}}, 'time': 4.56}\n",
      "{'verified': False, 'distance': 0.6957637720228249, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 68, 'h': 68}}, 'time': 5.05}\n",
      "{'verified': False, 'distance': 0.4646691698434975, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 190, 'h': 190}}, 'time': 4.69}\n",
      "{'verified': False, 'distance': 0.523989369205706, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 200, 'h': 200}}, 'time': 4.28}\n",
      "{'verified': False, 'distance': 0.6960963922255169, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 53, 'h': 53}}, 'time': 4.95}\n",
      "{'verified': False, 'distance': 0.5637995708812107, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 160, 'h': 160}}, 'time': 6.34}\n",
      "{'verified': False, 'distance': 0.6875611647185703, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 3024, 'h': 4032}, 'img2': {'x': 0, 'y': 0, 'w': 62, 'h': 62}}, 'time': 5.2}\n",
      "{'verified': False, 'distance': 0.685194468035663, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 62, 'h': 62}}, 'time': 5.26}\n",
      "{'verified': False, 'distance': 0.7000560993697991, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 54, 'h': 54}}, 'time': 5.31}\n",
      "{'verified': False, 'distance': 0.6925738397140176, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 51, 'h': 51}}, 'time': 5.49}\n",
      "{'verified': False, 'distance': 0.43243992472176984, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 209, 'h': 209}}, 'time': 4.95}\n",
      "{'verified': True, 'distance': 0.2846165912812546, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 218, 'h': 218}}, 'time': 4.66}\n",
      "{'verified': True, 'distance': 0.273337241454622, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 215, 'h': 215}}, 'time': 4.8}\n",
      "{'verified': True, 'distance': 0.31165527811637594, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 218, 'h': 218}}, 'time': 4.59}\n",
      "{'verified': True, 'distance': 0.3207882474108009, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 218, 'h': 218}}, 'time': 4.74}\n",
      "{'verified': False, 'distance': 0.6732157094930504, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 52, 'h': 52}}, 'time': 4.68}\n",
      "{'verified': False, 'distance': 0.6793720655167641, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 58, 'h': 58}}, 'time': 4.78}\n",
      "{'verified': False, 'distance': 0.4319792134267417, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 218, 'h': 218}}, 'time': 4.56}\n",
      "{'verified': False, 'distance': 0.7135618598204339, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 55, 'h': 55}}, 'time': 4.81}\n",
      "{'verified': False, 'distance': 0.7325485510895562, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 50, 'h': 50}}, 'time': 4.74}\n",
      "{'verified': False, 'distance': 0.6697048119368496, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 56, 'h': 56}}, 'time': 4.46}\n",
      "{'verified': False, 'distance': 0.7020454414863907, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 54, 'h': 54}}, 'time': 4.61}\n",
      "{'verified': False, 'distance': 0.6634155692605037, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 69, 'h': 69}}, 'time': 4.42}\n",
      "{'verified': False, 'distance': 0.6932396757842924, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 54, 'h': 54}}, 'time': 4.36}\n",
      "{'verified': False, 'distance': 0.4690904618101651, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 165, 'h': 165}}, 'time': 5.46}\n",
      "{'verified': False, 'distance': 0.7059319270507536, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 51, 'h': 51}}, 'time': 4.86}\n",
      "{'verified': False, 'distance': 0.7117173206047328, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 54, 'h': 54}}, 'time': 4.48}\n",
      "{'verified': True, 'distance': 0.2795584989093184, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 238, 'h': 238}}, 'time': 4.7}\n",
      "{'verified': True, 'distance': 0.26283608436045436, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 232, 'h': 232}}, 'time': 4.76}\n",
      "{'verified': False, 'distance': 0.693461133538307, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 54, 'h': 54}}, 'time': 4.37}\n",
      "{'verified': True, 'distance': 0.3038587237278657, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 236, 'h': 236}}, 'time': 4.88}\n",
      "{'verified': True, 'distance': 0.2644774480008485, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 233, 'h': 233}}, 'time': 4.9}\n",
      "{'verified': False, 'distance': 0.7428798271562507, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 57, 'h': 57}}, 'time': 7.4}\n",
      "{'verified': False, 'distance': 0.6766729003129921, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 56, 'h': 56}}, 'time': 6.38}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r9/lk69b51j6x96k8r1xj0vp4_h0000gp/T/ipykernel_3372/4064716282.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;31m# Draw a rectangle around the face and tag as 'Thats me!!'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Thats me!!\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Print any errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Display the resulting frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/deepface/DeepFace.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(img1_path, img2_path, model_name, detector_backend, distance_metric, enforce_detection, align, normalization)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mregions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# now we will find the face pair with minimum distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimg1_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg1_region\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg1_objs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimg2_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg2_region\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg2_objs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             img1_embedding_obj = represent(\n\u001b[0m\u001b[1;32m    178\u001b[0m                 \u001b[0mimg_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg1_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0menforce_detection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menforce_detection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/deepface/DeepFace.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(img_path, model_name, enforce_detection, detector_backend, align, normalization)\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[0;31m# represent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"keras\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# model.predict causes memory issue when it is called in a for loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;31m# embedding = model.predict(img, verbose=0)[0].tolist()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m             \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;31m# SFace and Dlib are not keras models and no verbose arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcopied_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mlayout_map_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_subclass_model_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layout_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1147\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 ):\n\u001b[0;32m-> 1149\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/keras/src/engine/functional.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \"\"\"\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/keras/src/engine/functional.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_id\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_dict\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_input_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                     \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# Node is not computable, try skipping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                 \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                 for x_id, y in zip(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1147\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 ):\n\u001b[0;32m-> 1149\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    286\u001b[0m             outputs = self._jit_compiled_convolution_op(\n\u001b[1;32m    287\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             )\n\u001b[1;32m    289\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0moutput_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, inputs, kernel)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mtf_padding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mtf_padding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         return tf.nn.convolution(\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"VALID\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m     \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m     name=None):\n\u001b[0;32m-> 1186\u001b[0;31m   return convolution_internal(\n\u001b[0m\u001b[1;32m   1187\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m       \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m       \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_conv3d_expanded_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m       return op(\n\u001b[0m\u001b[1;32m   1320\u001b[0m           \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m           \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m           \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2789\u001b[0m   \u001b[0minput_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2790\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minput_rank\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput_rank\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2791\u001b[0m     \u001b[0;31m# We avoid calling squeeze_batch_dims to reduce extra python function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2792\u001b[0m     \u001b[0;31m# call slowdown in eager mode.  This branch doesn't require reshapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2793\u001b[0;31m     return gen_nn_ops.conv2d(\n\u001b[0m\u001b[1;32m   2794\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2795\u001b[0m         \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2796\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \"dilations\", dilations)\n\u001b[1;32m   1339\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1342\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1343\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m       return conv2d_eager_fallback(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "import cv2\n",
    "\n",
    "# Load OpenCV's Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    if not ret:\n",
    "        break  # If there's an error or no frame, exit the loop\n",
    "\n",
    "    # Convert the frame to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Save the captured frame to a file\n",
    "        face_frame = frame[y:y+h, x:x+w]\n",
    "        cv2.imwrite('current_frame.jpg', face_frame)\n",
    "\n",
    "        try:\n",
    "            # Perform real-time face verification\n",
    "            result = DeepFace.verify(img1_path='./Downloads/profile_PIX_.JPG', img2_path='current_frame.jpg', enforce_detection=False)\n",
    "            print(result)\n",
    "            if result[\"verified\"]:\n",
    "                # Draw a rectangle around the face and tag as 'Thats me!!'\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, \"Thats me!!\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)  # Print any errors\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything's done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d58d1ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import cv2\n",
    "\n",
    "# Load a pre-trained face detection model from OpenCV\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    if not ret:\n",
    "        break  # If there's an error or no frame, exit the loop\n",
    "\n",
    "    # Convert frame to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "    # Save the captured frame to a file\n",
    "    frame_path = 'current_frame.jpg'\n",
    "    cv2.imwrite(frame_path, frame)\n",
    "\n",
    "    try:\n",
    "        # Perform real-time face verification\n",
    "        result = DeepFace.verify(img1_path='./Downloads/profile_PIX_.JPG', img2_path=frame_path, enforce_detection=False)\n",
    "        print(result)\n",
    "        # Handle verification result\n",
    "        for (x, y, w, h) in faces:\n",
    "            if result[\"verified\"]:\n",
    "                # Draw a rectangle around the face and tag as 'That's me!!'\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, \"That's me!!\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                # Display similarity score and distance\n",
    "                cv2.putText(frame, f\"Score: {result['score']:.2f}\", (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                cv2.putText(frame, f\"Distance: {result['distance']:.2f}\", (x, y+h+40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "            else:\n",
    "                # If the face is not verified, tag as 'New user'\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "                cv2.putText(frame, \"New patient\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                # Display similarity score and distance\n",
    "                cv2.putText(frame, f\"Score: {result['score']:.2f}\", (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                cv2.putText(frame, f\"Distance: {result['distance']:.2f}\", (x, y+h+40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)  # Print any errors\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything's done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7191bbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verified': False, 'distance': 0.5227662752645965, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 6.01}\n",
      "{'verified': False, 'distance': 0.4505313111874335, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 639, 'y': 274, 'w': 223, 'h': 223}}, 'time': 4.54}\n",
      "{'verified': False, 'distance': 0.4980159507057351, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 5.11}\n",
      "{'verified': False, 'distance': 0.497912924447077, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.8}\n",
      "{'verified': True, 'distance': 0.274102025343008, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 557, 'y': 303, 'w': 241, 'h': 241}}, 'time': 4.92}\n",
      "{'verified': True, 'distance': 0.35696052193826866, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 548, 'y': 313, 'w': 237, 'h': 237}}, 'time': 5.11}\n",
      "{'verified': False, 'distance': 0.7185221861916753, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 534, 'y': 249, 'w': 217, 'h': 217}}, 'time': 5.15}\n",
      "{'verified': False, 'distance': 0.5610805547487459, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.99}\n",
      "{'verified': False, 'distance': 0.5698541422417235, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 5.02}\n",
      "{'verified': False, 'distance': 0.5703051872359282, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.83}\n",
      "{'verified': False, 'distance': 0.5803922111317985, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.8}\n",
      "{'verified': False, 'distance': 0.5766988476592585, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.76}\n",
      "{'verified': False, 'distance': 0.5603868526278715, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 5.07}\n",
      "{'verified': False, 'distance': 0.4999520827812034, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 488, 'y': 249, 'w': 244, 'h': 244}}, 'time': 5.02}\n",
      "{'verified': False, 'distance': 0.4867298539607877, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 491, 'y': 256, 'w': 239, 'h': 239}}, 'time': 5.05}\n",
      "{'verified': False, 'distance': 0.5622651850804661, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 5.16}\n",
      "{'verified': False, 'distance': 0.5656733404977135, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 5.23}\n",
      "{'verified': False, 'distance': 0.5003653463675828, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 681, 'y': 286, 'w': 295, 'h': 295}}, 'time': 5.24}\n",
      "{'verified': False, 'distance': 0.5735028218443632, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.82}\n",
      "{'verified': False, 'distance': 0.4690461378506029, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 624, 'y': 286, 'w': 256, 'h': 256}}, 'time': 4.99}\n",
      "{'verified': False, 'distance': 0.5734645472612121, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 5.13}\n",
      "{'verified': False, 'distance': 0.4887665897984169, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 623, 'y': 300, 'w': 250, 'h': 250}}, 'time': 5.03}\n",
      "{'verified': False, 'distance': 0.5917056386497015, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.94}\n",
      "{'verified': False, 'distance': 0.5253009071751962, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 618, 'y': 299, 'w': 257, 'h': 257}}, 'time': 4.83}\n",
      "{'verified': False, 'distance': 0.5848437815233094, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.8}\n",
      "{'verified': False, 'distance': 0.581324436575014, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 5.12}\n",
      "{'verified': False, 'distance': 0.40422850360842255, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 737, 'y': 343, 'w': 240, 'h': 240}}, 'time': 5.09}\n",
      "{'verified': False, 'distance': 0.441810215895042, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 727, 'y': 335, 'w': 255, 'h': 255}}, 'time': 5.03}\n",
      "{'verified': False, 'distance': 0.48242845979498616, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 726, 'y': 347, 'w': 247, 'h': 247}}, 'time': 4.85}\n",
      "{'verified': False, 'distance': 0.49412010690836206, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 717, 'y': 362, 'w': 232, 'h': 232}}, 'time': 5.0}\n",
      "{'verified': False, 'distance': 0.4901010494184196, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 719, 'y': 369, 'w': 234, 'h': 234}}, 'time': 5.05}\n",
      "{'verified': False, 'distance': 0.4654109236437266, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 700, 'y': 369, 'w': 233, 'h': 233}}, 'time': 5.02}\n",
      "{'verified': False, 'distance': 0.5949882775099089, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.7}\n",
      "{'verified': False, 'distance': 0.623665146993579, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 4.8}\n",
      "{'verified': False, 'distance': 0.5925137303594352, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 834, 'y': 1336, 'w': 1540, 'h': 1540}, 'img2': {'x': 0, 'y': 0, 'w': 1280, 'h': 720}}, 'time': 6.6}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(frame_path, frame)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Perform real-time face verification\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mDeepFace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./Downloads/profile_PIX_.JPG\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (x, y, w, h) \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;66;03m# Initialize label and color (default to 'New user' and red)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/deepface/DeepFace.py:154\u001b[0m, in \u001b[0;36mverify\u001b[0;34m(img1_path, img2_path, model_name, detector_backend, distance_metric, enforce_detection, align, normalization)\u001b[0m\n\u001b[1;32m    151\u001b[0m target_size \u001b[38;5;241m=\u001b[39m functions\u001b[38;5;241m.\u001b[39mfind_target_size(model_name\u001b[38;5;241m=\u001b[39mmodel_name)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# img pairs might have many faces\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m img1_objs \u001b[38;5;241m=\u001b[39m \u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_faces\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg1_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdetector_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetector_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrayscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_detection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m img2_objs \u001b[38;5;241m=\u001b[39m functions\u001b[38;5;241m.\u001b[39mextract_faces(\n\u001b[1;32m    164\u001b[0m     img\u001b[38;5;241m=\u001b[39mimg2_path,\n\u001b[1;32m    165\u001b[0m     target_size\u001b[38;5;241m=\u001b[39mtarget_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m     align\u001b[38;5;241m=\u001b[39malign,\n\u001b[1;32m    170\u001b[0m )\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# --------------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/deepface/commons/functions.py:167\u001b[0m, in \u001b[0;36mextract_faces\u001b[0;34m(img, target_size, detector_backend, grayscale, enforce_detection, align)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     face_detector \u001b[38;5;241m=\u001b[39m FaceDetector\u001b[38;5;241m.\u001b[39mbuild_model(detector_backend)\n\u001b[0;32m--> 167\u001b[0m     face_objs \u001b[38;5;241m=\u001b[39m \u001b[43mFaceDetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_detector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetector_backend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# in case of no face found\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(face_objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m enforce_detection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/deepface/detectors/FaceDetector.py:81\u001b[0m, in \u001b[0;36mdetect_faces\u001b[0;34m(face_detector, detector_backend, img, align)\u001b[0m\n\u001b[1;32m     78\u001b[0m detect_face_fn \u001b[38;5;241m=\u001b[39m backends\u001b[38;5;241m.\u001b[39mget(detector_backend)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detect_face_fn:  \u001b[38;5;66;03m# pylint: disable=no-else-return\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_face_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_detector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# obj stores list of (detected_face, region, confidence)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/deepface/detectors/OpenCvWrapper.py:63\u001b[0m, in \u001b[0;36mdetect_face\u001b[0;34m(detector, img, align)\u001b[0m\n\u001b[1;32m     60\u001b[0m detected_face \u001b[38;5;241m=\u001b[39m img[\u001b[38;5;28mint\u001b[39m(y) : \u001b[38;5;28mint\u001b[39m(y \u001b[38;5;241m+\u001b[39m h), \u001b[38;5;28mint\u001b[39m(x) : \u001b[38;5;28mint\u001b[39m(x \u001b[38;5;241m+\u001b[39m w)]\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m align:\n\u001b[0;32m---> 63\u001b[0m     detected_face \u001b[38;5;241m=\u001b[39m \u001b[43malign_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meye_detector\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetected_face\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m img_region \u001b[38;5;241m=\u001b[39m [x, y, w, h]\n\u001b[1;32m     67\u001b[0m resp\u001b[38;5;241m.\u001b[39mappend((detected_face, img_region, confidence))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/deepface/detectors/OpenCvWrapper.py:108\u001b[0m, in \u001b[0;36malign_face\u001b[0;34m(eye_detector, img)\u001b[0m\n\u001b[1;32m    106\u001b[0m     left_eye \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m(left_eye[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m (left_eye[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)), \u001b[38;5;28mint\u001b[39m(left_eye[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (left_eye[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)))\n\u001b[1;32m    107\u001b[0m     right_eye \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m(right_eye[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m (right_eye[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)), \u001b[38;5;28mint\u001b[39m(right_eye[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (right_eye[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)))\n\u001b[0;32m--> 108\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mFaceDetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malignment_procedure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_eye\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_eye\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/deepface/detectors/FaceDetector.py:127\u001b[0m, in \u001b[0;36malignment_procedure\u001b[0;34m(img, left_eye, right_eye)\u001b[0m\n\u001b[1;32m    124\u001b[0m         angle \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m90\u001b[39m \u001b[38;5;241m-\u001b[39m angle\n\u001b[1;32m    126\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[0;32m--> 127\u001b[0m     img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img\u001b[38;5;241m.\u001b[39mrotate(direction \u001b[38;5;241m*\u001b[39m angle))\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/PIL/Image.py:693\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    691\u001b[0m                 warnings\u001b[38;5;241m.\u001b[39mwarn(e)\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 693\u001b[0m new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m], new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtypestr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m_conv_type_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/PIL/Image.py:229\u001b[0m, in \u001b[0;36m_conv_type_shape\u001b[0;34m(im)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# Modes\u001b[39;00m\n\u001b[1;32m    226\u001b[0m _ENDIAN \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mbyteorder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlittle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_conv_type_shape\u001b[39m(im):\n\u001b[1;32m    230\u001b[0m     m \u001b[38;5;241m=\u001b[39m ImageMode\u001b[38;5;241m.\u001b[39mgetmode(im\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    231\u001b[0m     shape \u001b[38;5;241m=\u001b[39m (im\u001b[38;5;241m.\u001b[39mheight, im\u001b[38;5;241m.\u001b[39mwidth)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "import cv2\n",
    "\n",
    "# Load a pre-trained face detection model from OpenCV\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Define a threshold for the face distance for your specific scenario\n",
    "threshold = 0.45\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    if not ret:\n",
    "        break  # If there's an error or no frame, exit the loop\n",
    "\n",
    "    # Convert frame to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "    # Save the captured frame to a file\n",
    "    frame_path = 'current_frame.jpg'\n",
    "    cv2.imwrite(frame_path, frame)\n",
    "\n",
    "    try:\n",
    "        # Perform real-time face verification\n",
    "        result = DeepFace.verify(img1_path='./Downloads/profile_PIX_.JPG', img2_path=frame_path, enforce_detection=False)\n",
    "        print(result)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Initialize label and color (default to 'New user' and red)\n",
    "            label = \"New user\"\n",
    "            color = (0, 0, 255)  # Red for 'New user'\n",
    "\n",
    "            # Check the result and update label and color if it's a match\n",
    "            if result.get(\"distance\") is not None and result[\"distance\"] <= threshold:\n",
    "                label = \"That's me!!\"\n",
    "                color = (0, 255, 0)  # Green for 'That's me!!'\n",
    "\n",
    "            # Draw a rectangle and label with the determined color\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "            cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "\n",
    "            # Display distance if available\n",
    "            if result.get(\"distance\") is not None:\n",
    "                cv2.putText(frame, f\"Distance: {result['distance']:.2f}\", (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)  # Print any errors\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything's done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daadebc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No faces found in the image!\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "\n",
    "image_path = './Downloads/img_test2.jpeg'\n",
    "\n",
    "# Load the image\n",
    "image = face_recognition.load_image_file(image_path)\n",
    "\n",
    "# Find all face locations in the image\n",
    "face_locations = face_recognition.face_locations(image)\n",
    "\n",
    "# If no faces are found, face_locations will be an empty list\n",
    "if not face_locations:\n",
    "    print(\"No faces found in the image!\")\n",
    "else:\n",
    "    # Assuming the first face in the image is the one we want to recognize\n",
    "    face_encoding = face_recognition.face_encodings(image, known_face_locations=face_locations)[0]\n",
    "    # Rest of your code...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec830617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dlib in ./opt/anaconda3/envs/openai/lib/python3.11/site-packages (19.24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade dlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e025e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(face_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95387488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
